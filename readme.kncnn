Description of Object-Oriented Neural Network (OONN) Builder package
(PC version)

ALL RIGHTS ARE RESERVED:

Under no circumstance will this software package be copied or distributed
by any means without the authorization of the provider.


OVERVIEW:

This program package contains software for building a feedforward
neural network of any configuration and initial weight setting 
for learning and inferencing. Prior knowledge can be built into
the network to implement a Knowledge-Based Conceptual Neural Network 
(KBCNN) (see Chapter 4 and [1]). If no knowledge is available, 
a general backpropagation network can be built.

The programs are written in C and compiled successfully by Borland C++ 3.0. 
The software can run on any PC (IBM PC) model.
The package is divided into three parts: (1) translation part, 
(2) learning and testing part, and (3) potential rule adding part. The 
translation part translates a document file with rules into neural net
structure. The learning and testing part is used to train the neural network 
(NN) and test the resulted NN after training. The potential rule adding part 
is for adding potential rules, i.e., potential nodes and connections, to the 
NN when needed.  The third module alone can build a general backpropagation 
network without prior knowledge.


The neural network structure information is put in an object (node) basis. 
Each processing element (PE) is viewed as an object which contains all the 
information about this node, including the class which the object belongs to, 
the object name, the output, the error, incoming connections, outgoing 
connections, ... etc. The signal forwarding works like a data-driven process 
and the error backpropagation is a goal-oriented process. This design is a 
must-do since the NN is not in a clear layer-by-layer structure. An 
intermediate conceptual PE can receive inputs from input PE's and all 
other intermediate conceptual PE's as long as there are no loops in the 
network structure.


OBTAINING SOURCES:

All the source files are put in 4 directories: HEADER, PCTRANS, PCKBCNN
and PCPRULE. You need to include the .h files in HEADER, then compile and
link the .c files under PCTRANS, PCKBCNN, and PCPRULE, respectively. We
suggest trans, kbcnn, and prule as the names of the executable files.


USE OF trans, kbcnn and prule:

(1) trans: (translation part)

    "trans" translates a document file with rules into neural net structure.
    The user has to specify the number and names of input conditions, 
    intermediate concepts, and output consequents in the rule file. The 
    number of rules and all rules in a specific format are also needed. 
    The rule format is

		A1 and A2 and not A3 and A4 ... then C Rule_Strength

    where Ai's can be an input condition or an intermediate concept and C 
    is an intermediate concept or an output consequent. A rule strength is
    attached at the end of each rule. It should be a number in the range
    [-1, 1].  0.5 is recommended for general rules without a specified 
    certainty factor.  Notice that "and", "not", and "then" are restricted 
    words and cannot be used as the name of any concept. And "not" is used 
    to negate a concept. 
    
    File trans_te.doc in directory TEMPLATE is a template for writing 
    a rule file. Fill in a proper value in each slot. 
    The value of the slot ``MaxNumberOfConditionsInEachRule"
    should be greater than or equal to the number of conditions of
    the largest possible rule in the problem domain.
    The ``NumberOfRules" slot much be filled in a value equal to
    the actual number of rules put in the file; else the program
    may not function properly.
    See also EXAMPLES\animal.doc for an example. 

    To run the program, simply type

    C:\> trans RULE_FILE_NAME (NN_FILE_NAME)

    If the user doesn't specify the output NN filename, the program set the 
    filename to "transout.nn." Each PE (node) is assigned a node ID number. 
    And the nodes are put in the order of input nodes, disjunction nodes 
    (intermediate concepts), output nodes and conjunction nodes (one for 
    each rule).


(2) kbcnn: (learning and testing part)

    In the learning and testing part, only part of the algorithm in [1] is 
    implemented. Sparse transformation, clustering of hidden PE's and second
    back propagation are left out. The user has to give a user-input file which
    specifies the mode ("0" for learning and "1" for testing), activation 
    function ("CF" for certainty factor model and "sigmoid" for sigmoid 
    function), learning rate, stop criteria by mean-square error and mean-
    square error fluctuation, maximum number of learning epochs, input file 
    name, desired output file name, first instance, last instance, initial NN 
    structure file name, final NN structure file name, mean-square error file 
    name, and test output file name. File TEMPLATE\usr.inp is a template file 
    for writing the user input file. See also EXAMPLES\knn.inp for an example.

    In discrete domains,
    if the activation function is the CF function, the input vector
    should be a vector of 1 and -1 and the output vector should be
    a vector of 1 and 0 in the data files;
    if the activation function is the sigmoid function, both the input 
    and the output vector should be a vector of 1 and 0.


    To run the program, type
 
    C:\> kbcnn USER_INPUT_FILE_NAME

    When it is under "learning mode", i.e., mode 0, the program will print 
    current number of epoch and mean-square error on the screen after each 
    epoch of learning. An epoch is feed-in of all input instances. The 
    program will stop when one of the stop criteria is satisfied or after 
    maximum number of epochs is reached. The NN structure after learning 
    is then saved in the final NN structure file. The user can use this 
    file as another initial NN structure file for further learning or 
    testing. The mean-square errors of all epochs are recorded in the 
    mean-square error file which is pre-specified in the usr input file. 
    The mean-square error file is appendable and can be use in the same 
    sequence of learning. 

    When it is under "testing mode", i.e., mode 1, the program will print MSE 
    and classification accuracy (in percentage) on the screen. If there are 
    mis-classified instances,  the numbers of those instance will also be shown.
    The outputs of the all test instances are saved in the test output file 
    which is also pre-specified in the user input file.

    After the neural network has been trained, it can be used for inference.
    That is, given an input pattern, it can generate an output pattern.
    To use for inference, simply set the neural network to the
    testing mode. But since the given instance or given set of
    instances has no desired output pattern, you just fill in
    zeros in the desired output file as if they were the desired outputs. 
    The network output should be recorded in the file specified
    under the slot ``OutputFileForTest" in the user input file (*.inp).

    When using the inference or test mode, you should modify the user
    input file (*.inp) as follows: the ``Mode" slot is filled in ``1",
    and the ``InitialStructureFile" slot filled in the
    trained neural network file name. 

(3) prule: (potential rule adding part)

    When the performance of the constructed NN is not good enough, potential
    rules need to be added to form a new NN structure. The user has to specify 
    the number and names of potential input, disjunction, output, and 
    conjunction nodes in a potential rule file. The number of connection 
    statements and all connection statements in a special format are also 
    needed. The format is

		A1 and A2 and A3 and A4 ... connect-to C1 and C2 ... #

    where Ai's are names of input, disjunction or conjunction nodes and Ci's 
    are are names of output, disjunction or conjunction nodes (either old or 
    newly added). The program will fully connect Ai's to Ci's. Notice that 
    "not" is not used here. A "#" is needed for the program to detect
    the end of each rule. File TEMPLATE\prule_te.doc is a template for writing 
    a potential rule file. Note that the values for the slots 
    ``MaxNumberOfLeftNodesInEachStatement" 
    and ``MaxNumberOfRightNodesInEachStatement"  can be changed to proper
    values. 
    The ``NumberOfConnectionStatements" slot much be filled in a value equal to
    the actual number of connection statements put in the file; else the program
    may not function properly.
    See also EXAMPLES\animalpr.doc for an example.

    To run the program, type

    C:\> prule POTENTIAL_RULE_FILE_NAME ORIGINAL_NN_FILE_NAME NEW_NN_FILE_NAME

    The program will add potential nodes with random biases and potential 
    connections with random weights to the original NN file and save to the 
    new NN file. The random numbers are set to be uniformly distributed in 
    [-0.1, 0.1]. If you want to change the distribution boundaries, you need 
    to change defined number "LIMIT" at line 20 of file add_con.c and 
    re-compile it.

    The program can be used to generated a new NN structure initialized with
    random biases and weights too. Simply specify an appropriate potential
    rule file. See Examples\xor.doc (for the exclusive-or function)
    or Examples\animalra.doc for an example. 
    Just type

    C:\> prule POTENTIAL_RULE_FILE_NAME NEW_NN_FILE_NAME

    Note that if you want to use the sigmoid function to run the 
    neural network for learning, each random initial weight 
    of the neural network may need to be 10 times as large as that generated
    by the program under the default limits. If this is the case,
    one can simply edit the neural network file using a text editor as follows:

	    ``0.0" is replaced by ``0." in the whole file


THE DIFFERENCE BETWEEN CONJUNCTION AND DISJUNCTION NODES:

A conjuction node is used to combine the inputs of all conditions of the 
antecedent (if) part of a rule. The initial bias of a conjunction node 
is set to be -0.2. 

A disjunction node represents an intermediate concept, which along with
output nodes can be used as the consequent (then) part of a rule. A
disjunction node has no bias.


HOW TO READ A NEURAL NET FILE:

In a neural net (structure) file, the first 4 numbers are the number of
input nodes, disjunction nodes, output nodes and conjunction nodes,
respectively. Then follows the information of each node. An input node 
example is

	node#3  input  fly
		 3    32 42 43 

where fly is the concept name of this node, 3 is the number of out
connections, 32, 42 and 43 are the node ID numbers of the nodes which
the out connections are connected to.

A disjunction node example is

	node#18  disjunction  mammal  0.0
		 2    29 0.479293 30 0.568097 
		 5    35 36 37 38 39 

where mammal is the concept name of this node, 0.0 is the bias, 2 is the
number of in connections, 29 and 30 are the node ID numbers of the nodes
where the in connections are from, 0.479293 and 0.568097 are the weight
strength of the two in connections, 5 is the number of out connections, 
and 35, 36, 37, 38, and 39 are the node ID numbers of the nodes which the 
out connections are connected to.

An output node example is

	node#23  output  cheetah  0.0
		  1   38 0.476743 

where cheetah is the concept name of this node, 0.0 is the bias, 1 is the
number of in connection, 38 is the node ID number of the node where the in
connections are from, 0.476743 is the weight strength of the in connection.

A conjunction node example is

	node#34  conjunction  conj6  -0.2
		 3    6 0.381809 7 0.293038 8 0.248441 
		 1    21 

where conj6 is the concept name of this node given automatically by the 
program, -0.2 is the bias of this node, 3 is the number of in connections,
6, 7, and 8 are the node ID numbers of the nodes where the in connections
are from, 0.381809, 0.293038 and 0.248441 are the weight strength of the
three in connections, 1 is the number of out connection, and 21 is the node
ID number of the node which the out connection is connected to.

Notice that an input node has no bias and in connections. An output node
has no out connections. The weight strength information is ONLY stored with
in connections, not with out connections, so that the same information won't
be stored twice.


EXAMPLES:

All the examples are put in directory EXAMPLES. See EXAMPLES/readme.doc for
detail.


OTHER INFORMATIONS:

(1) The package accepts network size up to 3000 nodes.
(2) The package accepts up to 10000 training or testing instances.
(3) All the concept names in a rule should be less than 20 characters.
(4) Rule files and potential rule files are named as XXX.doc.
(5) Neural net files are named as XXX.nn.
(6) User input files are named as XXX.inp.



REFERENCE:

[1] Fu, L. M., "Knowledge-Based Connectionism for Revising Domain Theories," 
    IEEE Transactions on Systems, Man and Cybernetics, vol. 23, no. 1, 
    pp. 173-182, 1993.


CONTRIBUTORS:

The algorithms were developed by Dr. LiMin Fu
and was reimplemented by Hui-huang Hsu in C language
under the direction of LiMin Fu.

DISCLAIMER:

The provider is not responsible for any malfunction of this software package.
